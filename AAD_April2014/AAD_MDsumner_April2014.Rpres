Tools for spatio-temporal data in R
========================================================
author: Michael Sumner
date: 14 March 2014
css: myslides.css
Southern Ocean Ecosystem Change Program, AAD
michael.sumner@aad.gov.au

![alt text][affil]
[affil]: affil.png "affil"

 
Summary
========================================================
- Tools in R for spatio-temporal data
- Extending R tools for AAD data
- Extensible extraction functions
- Map projections, frequently hidden
- Prospects - open development, web delivery with Shiny

The data repository
========================================================
- sea ice concentration
- chlorophyll-a
- sst
- wind vectors
- current vectors
- ssh/ssha
- model outputs [lon, lat, [depth], time]
- bathymetry

Extracting data for polygonal regions, boundaries, transects, point samples etc

Tool development captured in R packages

R Tools
========================================================
R and extension packages provide a wide variety of spatial tools
- Raster and vector, read/write: gdal, ncdf, 'native'
- Formal classes, metadata-rich objects
- Map projections and transformations
- Geometry manipulation, union, intersection, buffers

See the Spatial 'Task View' http://cran.csiro.au/web/views/Spatial.html

Raster tools for 2D and 3D grids
==========================================================
<small>
Low level 
- cellFromXY(x, pts)
- xyFromCell(x, cellnum)
- getValues(x, row)
- getValues(x, format = "matrix")
- adjacent(x, 10)
- writeRaster(x, filename, ...)

High level
- extent(x); dim(x); projection(x)
- merge, crop, projectRaster, aggregate, reclass, resample, rasterize, distance, focal, plotRGB, terrain ...
- Raster algebra: x + y; sqrt(y); log(x) etc. 
- Temporal or 'Z' axis for 3D

Regridding and extraction
- nearest-neighbour / bilinear-interpolation resampling
- aggregation/disaggregation to coarser/lower resolution
- warping (resampling grid points via projection transformation)

</small>



Example: Etopo2 and aggregated depth values
========================================================
<small>
- Read grid data and crop
- Extract aggregate grid values under polygon objects
- Reproject grid to polygons (for plotting)

```{r,eval=FALSE}
library(raster)
bb <- extent(25, 90, -70, -40)
topo <- crop(raster("Etopo2.tif"), bb)
topo[topo > -1] <- NA
```
```{r,eval=FALSE}
polys <- readOGR(".", "SSRUs")
polys$meantopo <- extract(topo, polys, fun = mean, na.rm = TRUE)
```

```{r,eval=FALSE}
## reproject the raster and plot both layers
projtopo <- projectRaster(topo, crs = projection(polys))
plot(projtopo)
incl <- !is.na(polys$meantopo)
plot(polys[incl,], add = TRUE, col = "#333333333B3")
text(coordinates(polys)[incl,], lab = format(polys$meantopo[incl], digits = 5), cex = 0.7)
```
</small>
<small>
Example: Etopo2 and aggregated depth values
========================================================
</small>
![alt text][etopo2plot]

[etopo2plot]: etopo2plot.png "Etopo2 and CCAMLR SSRUs"

R tools - time series grid
========================================================
High-level work flow also applies to time series grid data
<small>
```{r,eval=FALSE}
library(raster)
bb <- extent(100, 160, -70, -50)

## will leave bulk data on disk, read on demand as required
sst <- crop(brick("sst.wkmean.1990-present.nc"), bb, filename = "ReynoldsSST_subset.nc")

polys <- readOGR(".", "PolygonShapefile")

## matrix of [polyID,weeks]
(sstmatrix <- extract(sst, polys, fun = mean))
```
</small>
![alt text][reynoldssummary]

[reynoldssummary]: reynoldssummary.png "Reynolds SST summary"


R Tools - time-series data
========================================================
<small>
Raster can read from a variety of file types, and deal with 3D space-time grids, or assemble them from individual files
```{r,eval=FALSE}
library(raster)
dp <- "//aad.gov.au/files/AADC/Scientific_Data/Data/gridded/data"
sp <- "seaice/ssmi/ifremer/antarctic/daily/2013"
files <- c("20130719.nc", "20130726.nc", "20130802.nc", "20130809.nc", "20130816.nc", "20130823.nc", "20130830.nc", "20130906.nc")

fs <- file.path(dp, sp, files)

icedata <- stack(fs)
```
R AAD tools adds simplified helpers for rogue file collections:  
```{r,eval=FALSE}
library(raadtools)
dates <- seq(as.Date("2013-07-19"), by = "1 week", length = 8)
icedata <- readice(dates, product = "ssmi")
```
</small>
![alt text][icedata]
[icedata]: icedata.png "Ice data"

R AAD Tools - extensions to existing tools
========================================================
```{r,eval=FALSE}
(icedata <- readice(dates, product = "smmi"))
```

The raadtools package provides
- catalogue of available files in the repository
- normalization of input dates (unique, sort) match to repository dates
- read[x] function for product [x] with consistent structure
- record/fix incomplete/missing metadata, orientation, etc.
- flexible extraction methods, read slabs or extract point values

R Tools - data extraction
========================================================
Reading slabs is not necessarily what we want. 
<small>
```{r,eval=FALSE}
## read myPoints data                                  
myPoints <- read.table("fArchive.csv", sep = ",")
myPoints$datetime <- as.POSIXct(myPoints$datetime)
dcut <- cut(myPoints$datetime, by = "1 day")
levs <- levels(dcut)

## read[x] will normalize input dates
allice <- readice(myPoints$datetime)
for (i in seq_along(levs)) {
    ## match to today's ice
    ## ...
}
```
</small>

We probably can't load the entire time series first, and we probably shouldn't . . .

R Tools - data extraction
========================================================
Obviously it's smarter to only read today's slab: 
<small>
```{r,eval=FALSE}
myPoints$ice <- as.numeric(NA)
for (i in seq_along(levs)) {  
  ## match this day's myPoints data to the ice
  isub <- myPoints[levs == levs[i], c("long", "lat")]  
  
  ice <- readice(isub$datetime[1L])  
  vals <- extract(ice, isub, method = "bilinear")
  myPoints$ice[levs == levs[i]] <- vals
}
```

This is getting there but
- cannot extract at longlat because the grid is in Polar Stereographic
- need extra handling to interpolate in time
- perhaps aggregate the grid to a lower resolution
- code required starts to get complicated
- and we need to write this for each data type . . .

</small>


R  Tools - extract methods
========================================================

<small>
The raster construct for extracting data from grids: 
```{r,eval=FALSE}
library(raster)
r <- raster("myMegaSplat.grd")
extract(r, [query])
```
Where, [query] is:
- a data.frame/matrix of coordinates (must match projection of raster)
- SpatialPoints\*, SpatialLines\*, SpatialPolygons\* (reprojection will be performed if required)

It provides options for extraction, and we can aggregate into geometries
```{r,eval=FALSE}
extract(x, query, fun = mean)

extract(x, query, fun = function(x) dowhatever(x))
```
- if raster is 2D, we get value/s for each point/line/polygon
- if raster is 3D, we get a *matrix* of values for each point/line/polygon at each time step 

</small>

R AAD Tools - read functions for various data products
=========================================================
The raadtools package applies extract() to functions, with query a table of longitude, latitude, date-time: 
<small>
```{r,eval=FALSE}
xyt <- c("long", "lat", "datetime")
## extract                                                                                              
myPoints$iceconcentration <- extract(readice, myPoints[,xyt])
```
~~Geek alert: raadtools adds *methods* for the S4 *generic* raster::extract()~~

We can do the same with other read functions, and provide options: 
```{r,eval=FALSE}
myPoints$uCurr <- extract(readcurr, myPoints[,xyt], uonly = TRUE)
myPoints$vCurr <- extract(readcurr, myPoints[,xyt], vonly = TRUE)

myPoints$interpsst <- extract(readsst, myPoints[,xyt], ctstime = TRUE, method = "bilinear")

myPoints$regridSSHA <- extract(readssh, myPoints[,xyt], fact = 8, ssha = TRUE)
```

This is *extensible*: by adding new functions we can use those, without rebuilding/reinstalling the package providing the extract method. 

Similar extraction methods are possible for other scenarios. 
</small>


R AAD Tools - read / extract 
========================================================

<small>

If *readwhatever()* knows the grid, projection, time axis, catalogue, *extract* just applies the same engine to that. 

- readchla: weekly/monthly, SeaWIFS/MODIS, oceancolor/Robert Johnson
- readcurrents: weekly AVISO surface currents
- readfastice: East Antarctic Fast Ice Coverage, 2000-2008
- readice: daily/monthly, NSIDC/SSMI, southern hemisphere
- readfronts: weekly Sokolov/Rintoul frontal regions
- readmld: montly climatology Sallee mixed layer depth, Southern Ocean
- readrapid_response: daily RGB MODIS image, southern hemisphere
- readssh: weekly Sea Surface height/anomaly, global
- readsst: daily/monthly SST, global
- readwind: weekly surface wind vectors, global
- readtopo: Gebco 2008, IBCSO, Etopo1/2, Kerguelen, George V Terre Adelie, Smith and Sandwell

</small>

A Map projection
========================================================
<small>
- ... is a systematic transformation of the latitudes and longitudes of locations on the surface of a sphere or an ellipsoid into locations on a plane. 
- <b>All map projections distort</b> the surface in some fashion.
- <b>Depending on the purpose</b> of the map, <b>some distortions are acceptable and others are not</b>; therefore different map projections exist in order to preserve some properties of the sphere-like body <b>at the expense of other properties</b>.
- <b>There is no limit to the number of possible map projections</b>

</small>

http://en.wikipedia.org/wiki/Map_projection

R AAD Tools - data and projections
========================================================
<small>NSIDC data in native form, matrix [332,316], map units in kilometres: </small>

```{r,eval=TRUE,echo=FALSE}
graticule <- function (easts, norths, ndiscr = 100, proj = NULL) 
{
    bb = bbox(as.matrix(expand.grid(easts, norths)))
    ##easts <- easts[easts > bb[1, 1] & easts < bb[1, 2]]
    eastlist <- vector(mode = "list", length = length(easts))
    for (i in 1:length(easts)) eastlist[[i]] <- Line(cbind(rep(easts[i], 
        ndiscr), seq(bb[2, 1], bb[2, 2], length.out = ndiscr)))
    ##norths <- norths[norths > bb[2, 1] & norths < bb[2, 2]]
    northlist <- vector(mode = "list", length = length(norths))
    for (i in 1:length(norths)) northlist[[i]] <- Line(cbind(seq(bb[1, 
        1], bb[1, 2], length.out = ndiscr), rep(norths[i], ndiscr)))
    x <- SpatialLines(list(Lines(northlist, "NS"), Lines(eastlist, 
        "EW")), CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"
))
    if (!is.null(proj)) {require(rgdal);x <- spTransform(x, CRS(proj))}
    x
}

```{r,echo=FALSE,fig.height=8,fig.width=8,fig.align='center'}
library(raadtools)
date <- "2012-06-01"
x <- readice(date)
xmin(x) <- xmin(x)/1000
xmax(x) <- xmax(x)/1000
ymin(x) <- ymin(x)/1000
ymax(x) <- ymax(x)/1000
gl <- graticule(easts = seq(-180, 180, by = 30)[-c(4, 7, 9)], norths = seq(-80, -40, by = 10), proj = gsub("units=m", "units=km", projection(x)))
par(xaxs="r",yaxs= "r")
plot(x, main = sprintf("%s %s", "NSIDC sea ice concentration", date), border = FALSE)
box(col = "white")
plot(gl, add = TRUE, lty = 2,
     , col = "#333333A3")
grid(col="#333333B3", lty = 1)
```
<small> ~~solid line is km-grid, dash line is graticule~~</small>

NSIDC ice - a Polar projection
========================================================
This is a (south Polar) Stereographic projection on the WGS84 datum, with 
<small>
- central longitude 0.0
- central latitude -90.0
- true-scale latitude -71.0
- units of kilometres 
- (false easting/northing both 0.0, hence [0,0] is middle of the page, at south pole)
- <b>conformal,</b> i.e. good for <b>preserving the *shape* of objects locally</b> <small>(though *not* for calculating area over large regions . . .)</small>

PROJ.4

+proj=stere +lat_0=-90 +lat_ts=-71 +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=km +no_defs +ellps=WGS84 +towgs84=0,0,0"
</small>

Metadata allows treating this layer like a GIS would, for display, manipulation, extraction of data and conversion to other forms. 

Data and map projections
=========================================================
![alt text][mercator1]

[mercator1]: mercator1.png "AVISO surface currents"

This is what some GIS packages see from AVISO ocean currents stored in NetCDF.

Reorient the grid, looks right, but map doesn't line up . . .

Mercator-grid, longlat vs native
=========================================================
![alt text][mercator2]


[mercator2]: mercator2.png "AVISO surface currents - in a World Mercator"

<small>
PROJ.4
+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +over +units=m +no_defs

- Left panel is  *rectilinear*, latitude lines are denser towards the poles. 
- Right panel is *regular in native projection*, and this allows straightforward usage (and fast extraction methods). 

</small>

Which projection?
=========================================================

A global projection with acceptable compromises for all purposes is impossible. <small>Area, shape, distance and angle cannot be all balanced at once.</small> 

Projections for smaller areas provide much easier choices with less drastic compromise, e.g. polar regions


- There is no easy or right answer, but disguising these compromises rarely helps 
- Longitude and latitude arrays add unnecessary complexity and complication


Work with the coordinate system as delivered, transform queries to/from the native projection


Other map projections in common use
=========================================================
<small>
<b>Spherical Mercator</b> Smith/Sandwell topography-bathymetry
<small>+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs</small>

<b>WGS84 Mercator</b> AVISO currents, SSH/SSHA, Sokolov/Rintoul front zones
<small> +proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs</small>

<b> Equal Area Cylindrical</b> Fraser / Massom fast ice
<small>+proj=cea +lon_0=91 +lat_0=-90 +lat_ts=-65 +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0</small>
<br><small>+proj=cea +ellps=WGS84</small>

<b>Lambert Azimuthal Equal Area </b> CCAMLR management zone shapefiles
<small>+proj=laea +lat_0=-90 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m
+no_defs +ellps=WGS84 +towgs84=0,0,0</small>

<b> (North Polar) Stereographic </b> NSIDC, SSMI products,  North-Hemi Snow Cover Extent
<small> +proj=stere +lat_0=90 +lat_ts=70 +lon_0=-45 +k=1 +x_0=0 +y_0=0 +a=6378273 +b=6356889.449 +units=m +no_defs</small>
<br><small>+proj=stere +lat_0=90 +lon_0=10 +ellps=WGS84</small>

<b> (South Polar) Stereographic on the sphere</b> Arrigo/Dijken primary production 
<small>+proj=stere +lat_0=-90 +lon_0=180 +ellps=sphere</small>
 
<b> Longitude / latitude on WGS84</b> (no kidding!)
<small>
+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0
</small>

</small>

Conformal? 
========================================================

<img src="Conformal.png" height="1000px" width="1000px" />

R code
==========================================================
<small>
```{r,eval=FALSE}
library(raadtools)
library(rgdal)
library(maptools)
data(wrld_simpl)
## prepare base data, transform copies to LAEA for comparison
xst <- readice()
library(rgdal);library(maptools);data(wrld_simpl)
wst <- spTransform(wrld_simpl, CRS(projection(xst)))
xla <- projectRaster(xst, crs = gsub("stere", "laea", projection(xst)))
wla <- spTransform(wrld_simpl, CRS(projection(xla)))
## extents for both
est <- extent(xst) + 1.2e7
lst <- extent(xla) + 1.2e7

## plot
par(mfcol = c(1, 2), cex = 1.5)
plot(est, col = "white", axes = FALSE, xlab = "", ylab = "", asp = 1, main = "Stereographic")
plot(xst, add = TRUE, legend = FALSE)
plot(wst, add = TRUE)
plot(lst, col = "white", axes = FALSE, xlab = "", ylab = "", asp = 1, main = "Lambert Eq Area");
plot(xla, add = TRUE, legend = FALSE)
plot(wla, add = TRUE)
```

</small>


Other code 
========================================================
Work with the coordinate system as delivered, transform to/from the native projection

<small>
Not this! (Note, this approach looks the same in any language). 

```{r,echo=FALSE}
o <- options(width=120)
```
```{r,eval=FALSE,tidy=FALSE}
##' load data arrays from binary files
nn = 209091
fid = open('data.bin','rb')
x=readBin(fid,"integer",n=nn,size=1)
close(fid)
##' load lons/lats
fid = open('lats','rb')
lat=readBin(fid,"integer",n=nn,size=4)/1e5
close(fid)
fid = open('lons','rb')
lon=readBin(fid,"integer",n=nn,size=4)/1e5
close(fid)

##' area-cell or point-cell?
##' ?? ...
```
```{r,echo=FALSE}
options(o)
```
</small>

Open development
========================================================
- RStudio server
- users log-in via web browser
- interface identical to RStudio, shared R installation, shared network, etc. 
- streamlining tools to update/synchronize the file collections

![alt text][rstudio-web2]


[rstudio-web2]: rstudio-web2.png "RStudio"

Open development
========================================================
![alt text][rstudio-web]


[rstudio-web]: rstudio-web.png "RStudio"


Deploying to the web with Shiny
========================================================
<small>
Shiny lets us write very simple R code, with a little bit of wrapping and spin it up in a browser: 
```{r,eval=FALSE}
list.files("shinyrun")
##[1] "server.R" "ui.R" 
## server.R                                                           
library(shiny)
library(raadtools)
strf <- "NSIDC_%d_%b_%Y"
shinyServer(function(input, output) {
  output$rasterPlot <- renderPlot({
   dates <- input$dateRange
   x <- readice(dates)
   names(x) <- format(getZ(x), strf)
   plot(x)
  })})
## ui.R
library(shiny)
fs <- icefiles()
mindate <- as.Date(min(fs$date))
maxdate <- as.Date(max(fs$date))
shinyUI(pageWithSidebar(
  headerPanel("NSIDC daily sea ice"),
  sidebarPanel(dateRangeInput("dateRange", "Dates:",
      start = mindate, end = maxdate, 
      min = mindate, max = maxdate, separator = "and")
  ),
    mainPanel(plotOutput("rasterPlot"))))
```
</small>

Deploy on a server 
==========================================================================
(such as "localhost")

<small>
```{r,eval=FALSE,tidy=TRUE}
## load the shiny package and point  to a simple app
library(shiny)

dp <- "//aad.gov.au/files/Transfer"

runApp(file.path(dp,"shinyrun"))

```
</small>



Extract data for points with Shiny app
========================================================
```{r,eval=FALSE}
library(shiny)
runApp("shinyExtract")
```
![alt text][shinyApp0]


[shinyApp0]: shinyApp0.png "shinyApp0"

Extract data for points with Shiny app
========================================================

![alt text][shinyApp1]


[shinyApp1]: shinyApp1.png "shinyApp1"

Extract data for points with Shiny app
========================================================

![alt text][shinyApp2]


[shinyApp2]: shinyApp2.png "shinyApp2"

What next?
========================================================
<small>
- extend extract(function, query) for polygons/lines with time-range / time-spacing
- file catalogue is clunky, need tools (Ben Raymond has the solution)
- relation to other data portals?
- extend extract model to trip/track objects, tools for merging with track-estimates
- host the tools on fast, accessible hardware

Many thanks to long-suffering colleagues for conversations and testing. 

Interested users, testers, developers, collaborators?  michael.sumner@aad.gov.au

http://www.soki.aq/x/eolw
</small>

![alt text][affil]
[affil]: affil.png "affil"


Structure of read functions (1)
========================================================
<small><small>
```{r,eval=FALSE}
##' Read Polar model data.
##'
##' Polar biology model, on Stereographic grid.
##' @title Polar data
##' @param date date or dates of data to read
##' @param returnfiles if TRUE return just the files from \code{polarfiles}
##' @param time.resolution choice of temporal resolution, weekly only
##' @param xylim crop or not
##' @param ... ignored
##' @return RasterLayer or RasterBrick
##' @export
readpolar <- function(date,  time.resolution = "weekly", xylim = NULL, returnfiles = FALSE, ...) {
      ## private function to read a single file/time slice specific to this product
      read0 <- function(x) {
        proj <- "+proj=stere +lat_0=-90 +lon_0=180 +ellps=sphere"
        offset <- c(5946335, 5946335)
        dims <- c(1280L, 1280L)
        pdata <- readBin(x, numeric(), prod(dims), size = 4, endian = "little")
        pdata[pdata < 0] <- NA
        x <- list(x = seq(-offset[1L], offset[1L], length = dims[1L]),
                   y =  seq(-offset[2L], offset[2L], length = dims[2L]),
                   z = matrix(pdata, dims[1L])[rev(seq_len(dims[2L])),])
        raster(x, crs = proj)

    }
    ## process input options
    time.resolution <- match.arg(time.resolution)
    files <- polarfiles()
    if (returnfiles) return(files)

    ## provide a default for readpolar() with no input
    if (missing(date)) date <- min(files$date)
    date <- timedateFrom(date)
    
    
   ## continued . . . 
   
    
    
    }
```
</small></small>

Structure of read functions (2)
========================================================
<small><small>
```{r,eval=FALSE}
  
  ## readpolar <- function(date,  time.resolution = "weekly", xylim = NULL, returnfiles = FALSE, ...) {

  ## cont.

    ## normalize input to file dates
    files <- .processFiles(date, files, time.resolution)

    ## crop if requested
    cropit <- FALSE
    if (!is.null(xylim)) {
        cropit <- TRUE
        cropext <- extent(xylim)
    }

    ## collect individual slices and bundle as a RasterBrick
    nfiles <- nrow(files)
    r <- vector("list", nfiles)
    for (ifile in seq_len(nfiles)) {
        r0 <- read0(files$fullname[ifile])
        if (cropit) 
            r0 <- crop(r0, cropext)
        r[[ifile]] <- r0
    }
    r <- brick(stack(r))

    ## tidy up the names and the 3rd axis metadata
    names(r) <- sprintf("polar_%s", format(files$date, "%Y%m%d"))
    setZ(r, files$date)
}

```
</small></small>

